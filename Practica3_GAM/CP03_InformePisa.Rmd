---
title: "CP03 Informe PISA"
author: "Isabel Afán de Ribera"
date: "8/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción del dataset

El conjunto de datos se ha construido utilizando la puntuación media en Ciencias por país del Programa para la Evaluación Internacional de Estudiantes (PISA) 2006, junto con el GNI per cápita (paridad del poder adquisitivo, dólares de 2005), el índice educativo, el índice de salud y el índice de desarrollo humano de la ONU (HDI). El dataset se compone de 52 observaciones y 11 variables, las variables claves son las siguientes:

* `Overall Science Score (average score for 15 year olds)`
* `Interest in science`
* `Support for scientific inquiry`
* `Income Index`
* `Health Index`
* `Education Index`
* `Human Development Index (composed of the Income index, Health Index, and Education Index)`

# Objetivo 

El objetivo del presente trabajo es modelizar la relación entre la puntuación media (OSS) y el resto de variables, utilizando modelos de splines y GAM.

# Librerias y funciones

```{r librerias y funciones, message=FALSE, warning=FALSE}

library(janitor) # limpieza de nombres
library(magrittr) # pipe operators
library(skimr) # summarize
library(imputeTS) # NA mean
library(tidyverse)
library(mgcv) # estimar GAM
library(gam) # modelo GAM
library(reshape2) # melt
library(splines) # calculo de splines
library(ggplot2) # visualizacion

```

# Lectura de datos 

```{r datos}

pisa = read.csv('data/pisasci2006.csv')
colnames(pisa)

```
# Nombres de variables

```{r clean names}

pisa %<>% clean_names()
colnames(pisa)
# renombramos las variables para ponerlas en minusculas

```
# Summarize Data

```{r Summarise Data}

skim(pisa)
# obtengo estadísticos descriptivos de las variables de tipo character (country) y de tipo numeric (el resto) como la media, la desviación tipica, los cuartiles o el numero de valores nulos. Además, obtengo histogramas de todas las variables numericas para conocer un poco mas las variables con las que voy a trabajar

```
# Limpieza de datos

```{r Data Wranling}

# delete duplicate
# remove duplicate rows of the dataframe
pisa %<>% distinct(country,.keep_all= TRUE)
# %<>% para que lo de la izquierda entre tambien en la funcion

# NA mean, en esta ocasion evitamos eliminar filas con NA ya que hay pocas observaciones y puede perderse mucha informacion
pisa %<>% na_mean()

# Summarise
skim(pisa)

# hemos limpiado los datos eliminando duplicados, calculado la media de los valores nulos y hemos repetido summarise con los datos limpios

```
# Modelos aditivos generalizados GAM

## Representación gráfica de cada variable con respecto a overall

```{r}

data_melt = pisa %>%  # funcion melt del paquete reshape2 para convertir el dataframe y poder visualizar la relacion de cada variable explicativa con la variable dependendiente overall
  select(-evidence, -explain, -issues) %>%  # hemos incluido en la visualizacion solo las variables relevantes; evidence, explain e issues siguen una relacion lineal
  gather(key=Variable, # en el nuevo df aparece una columna relativa a la variable y otra a su valor, además de overall y country
         value=Value, 
         -overall, -country)

ggplot(aes(x = Value,y = overall), data = data_melt) +
  geom_point(color = 'goldenrod1',alpha = 0.75) +      # nube de puntos
  geom_smooth(se = F, lwd = 0.6, color = 'darkslategray3') +  # linea de regresion lineal suavizador por defecto loess
  geom_text(aes(label = country), alpha = 0, size = 1,angle = 30, hjust = -.2,
            vjust = -.2) +
  facet_wrap(~Variable, scales='free_x') +
  labs(x = '') +
  theme_bw()

```
Las variables independientes edu, hdi, health e income tienen correlación positiva con la variable dependiente overall con la particularidad de que llega un punto en el que la variable income se estabiliza en los 0.8 puntos y comienza a bajar convirtiendose la correlación en negativa. En el caso de las variables interest y support se da una correlación negativa con overall.

## Splines

Un spline es una curva diferenciable definida en porciones mediante polinomios.

Pasamos a ajustar el spline con un spline de 10 grados de libertad y con ello obtenemos el spline de cada una de las variables independientes mediante cross validation. La forma más sencilla de ajustar splines es mediante la función smooth.spline, en las smooth splines no se tiene que elegir el número y posición de los knots, ya que hay uno en cada observación.

### edu vs overall
```{r spline edu vs overall}

plot(pisa$edu, pisa$overall, xlim=pisa$eduLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$edu, pisa$overall, df=10) # la funcion smooth.spline() permite ajustar smooth splines, y encuentra el grado de libertad optimo mediante cross validation
# primero ajustamos con 10 grados de libertad
fit2 <- smooth.spline(pisa$edu, pisa$overall, cv=TRUE) # ajustamos cons cross validation
fit2$df # grados de liberdad variable edu 2.002

# representacion
lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', '2.002 DF'),                                            col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
### hdi vs overall
```{r spline hdi vs overall}

plot(pisa$hdi, pisa$overall, xlim=pisa$hdiLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$hdi, pisa$overall, df=10) # ajuste con grados de libertad
fit2 <- smooth.spline(pisa$hdi, pisa$overall, cv=TRUE) # cross validation
fit2$df # grados de liberdad variable hdi 8.603

# representamos
lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', ' 8.603 DF'),                                           col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
### health vs overall
```{r spline health vs overall}

plot(pisa$health, pisa$overall, xlim=pisa$healthLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$health, pisa$overall, df=10) # ajuste con grados de libertad
fit2 <- smooth.spline(pisa$health, pisa$overall, cv=TRUE) # cross validation
fit2$df # grados de liberdad variable health 2.002

# representamos
lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', '2.002 DF'),                                            col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
### income vs overall
```{r spline income vs overall}

plot(pisa$income, pisa$overall, xlim=pisa$incomeLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$income, pisa$overall, df=10) # ajuste con grados libertad
fit2 <- smooth.spline(pisa$income, pisa$overall, cv=TRUE) # cross validation
fit2$df # grados de liberdad variable income 4.244

lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', '4.244 DF'),                                            col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
### interest vs overall
```{r spline interest vs overall}

plot(pisa$interest, pisa$overall, xlim=pisa$interestLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$interest, pisa$overall, df=10) # ajuste con grados libertad
fit2 <- smooth.spline(pisa$interest, pisa$overall, cv=TRUE) # cross validation
fit2$df # grados de liberdad variable interest 4.75

lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', '4.75 DF'),                                             col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
### support vs overall
```{r spline support vs overall}

plot(pisa$support, pisa$overall, xlim=pisa$supportLims, col='gray') 
title('Smoothing Spline') 
fit <- smooth.spline(pisa$support, pisa$overall, df=10) # ajuste con grados libertad
fit2 <- smooth.spline(pisa$support, pisa$overall, cv=TRUE) # cross validation
fit2$df  # grados de liberdad variable support 2.001

lines(fit, col='darkslategray3', lwd=2) 
lines(fit2, col='goldenrod1', lwd=1) 
legend('topright',legend=c('10 DF', '2.001 DF'),                                            col=c('darkslategray3','goldenrod1'), lty=1,lwd=2,cex=0.8)

```
## Regresión local

Mediante la función loess ajustamos con un intervalo de 0.2 a 0.6, cada bloque consta del 20% o 60% de las observaciones.

```{r}

ggplot(data=pisa, aes(x=edu, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()

ggplot(data=pisa, aes(x=hdi, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()

ggplot(data=pisa, aes(x=health, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()

ggplot(data=pisa, aes(x=income, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()

ggplot(data=pisa, aes(x=interest, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()

ggplot(data=pisa, aes(x=support, y=overall)) + geom_point(color='gray') + 
  geom_smooth(method='loess', span=0.2) +  # metodo loess para la curva
  geom_smooth(method='loess', span=0.6, color='goldenrod1') + # span para la suavidad intervalo de 0.2 a 0.6
  theme_bw()
```


## GAM

Extensión del modelo lineal simple, se trata de adaptar un modelo a la respuesta Y sobre la base de varios predictores.

A continuación pasamos a establecer el modelo GAM usando la libreria gam de R con el objetivo de predecir el overall usando un smoothing spline para cada uno de los predictores del modelo (edu, hdi, health, income, interest)

```{r Modelo GAM}

gam_model <- gam(overall ~ s(edu, df=2.002) + s(hdi, df=8.603) + s(health, df=2.002) + s(income, df=4.244) + s(interest, df=4.75) + s(support, df=2.001), data = pisa)
# la s hace referencia a smoothing spline

# summary del modelo
summary(gam_model)
# genera un tabla resumen del ajuste 

```
La seccion ANOVA (analisis de varianza) examina los efectos tanto parametricos como no parametricos mostrando los p-value para cada predictor. Los p-values no parametricos se corresponden con el contraste de hipótesis (H0) de que la relación entre predictor y variable respuesta es lineal, frente a la alternativa de que no lo es (H1). En este caso, para los efectos no parametricos parecen significativos (income e interest). En todos ellos se rechazaría la H0 no hay una relación lineal entre los predictores y la variable a explicar overall. Para el resto de predictores podría resultar más conveniente emplear un ajuste lineal en lugar de smooth spline.

```{r graficos}

# representacion grafica de modelo GAM para cada variable
par(mfrow=c(1,3)) 
plot(gam_model, se=TRUE, col='darkslategray3')

```

En la interpretación de estos gráficos podemos concluir. Por un lado, manteniendo el resto de variables constantes la variable respuesta overall aumenta con edu, con health, con support y con income, en este último caso, hasta llegar al punto 0.8 donde se estabilizaría y empezaría a reducirse overall al aumentar esta variable. Por otro lado, manteniendo constantes el resto de variables la variable respuesta overall tiende a ser menor para valores mayores de hdi e interest.

### Comparación de modelos

Como anteriormente comentamos para los predictores que resultaban no significativos podría resultar más conveniente emplear un ajuste lineal en lugar de smooth spline. Por tanto, vamos a pasar a comparar modelos con distinto nivel de complejidad, llamamos gam1 al modelo resultante de seleccionar solo los predictores significativos; gam2 al modelo que utiliza una función lineal para las variables no significativas y gam3 referido a nuestro modelo original al que hemos aplicado smooth spline. Esta comparación la hacemos con el método ANOVA.


```{r comparando modelos}
gam1 <- gam(overall ~ s(income, df=4.244) + s(interest, df=4.75), data=pisa)

gam2 <- gam(overall ~ edu + hdi + health + support + s(income, df=4.244) + s(interest, df=4.75), data = pisa)

gam3 <- gam(overall ~ s(edu, df=2.002) + s(hdi, df=8.603) + s(health, df=2.002) + s(income, df=4.244) + s(interest, df=4.75) + s(support, df=2.001), data = pisa)

anova(gam1, gam2, gam3, test='F')
```

En base al p-value parece que el modelo más recomendable es el modelo 2, el cual utiliza una función lineal para las variables no significativas. Podemos concluir, como ya vimos con nuestro modelo original, que las variables edu, hdi, health, support no contribuyen al modelo.

Por último, ajustamos nuestro modelo original (gam_model) a la regresión local a través de la función lo() y span para el suavizado.

```{r local regression}

summary(gam_model)

gam_model <- gam(overall ~ s(edu, df=2.002) + s(hdi, df=8.603) + s(health, df=2.002) + s(income, df=4.244) + s(interest, df=4.75) + s(support, df=2.001), data = pisa)

gam.lo <- gam(overall ~ s(edu, df=2.002) + s(hdi, df=8.603) + s(health, df=2.002) + lo(interest, span=0.7) + lo(income, span=0.7) + s(support, df=2.001) ,data=pisa)
plot(gam.lo, se=TRUE, col='green')
```

# Conclusiones

De nuestro análisis puede concluirse que las variables más adecuadas para predecir la variable independiente overall son las variables explicativas income e interest.Y que el mejor modelo predictivo, en este caso, es aquel que emplea una función lineal para las variables edu, hdi, health y support y una función no lineal para income e interest con grados de libertad 4.24 y 4.75 respectivamente.


# References

* Generalized Additive Models (2019). Disponible en: https://m-clark.github.io/generalized-additive-models/application.html#multiple-predictors

* GitHub https://github.com/m-clark/generalized-additive-models-workshop-2019/tree/master/notebooks

* Métodos de regresión no lineal: Regresión Polinómica, Regression Splines, Smooth Splines y GAMs. Disponible en: https://rpubs.com/Joaquin_AR/250069

* Queralt, R.(2020). Apuntes asignatura Predicción. Regresión III GAM Models.




