---
title: 'CP02: Salarios NBA (CV y Regularización)'
author: "Isabel Afán de Ribera"
date: "5/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objetivo 

A partir del dataset nba aplicar Cross Validation y Regularización (Ridge, Lasso y Elastic Net) para obtener las variables que mejor explicar la variable dependiente salary.

# Librerias y funciones

```{r Libraries and functions, message=FALSE, warning=FALSE}
library(here) # Carga de datos
library(tidyverse)
library(janitor) # Limpieza nombre de variables
library(skimr) # Summarize
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(magrittr) # Pipe operators
library(rsample) # Division de los datos
library(glmnet) # Regularizacion
library(dplyr) # Manipulacion de datos
library(ggplot2) # Visualizacion
```

# Lectura de datos

```{r Read Data}
raw_data <-  read.csv("nba.csv")
colnames(raw_data)
```

# Variables
```{r}
raw_data %<>% clean_names()
colnames(raw_data)
# renombramos las variables para ponerlas en minusculas y sin puntos
```

# Resumen de los datos
```{r Summarise Data}
skim(raw_data)

# hay 3 variables de tipo caracter y las otras 25 son de tipo numerico. Aqui puede verse un resumen de sus medidas de posicion a traves de estadisticos descriptivos, incluyendo historgramas de las variables numericas
```

# Limpieza de datos
```{r Data Wranling}
# delete duplicate
# Remove duplicate rows of the dataframe
raw_data %<>% distinct(player,.keep_all= TRUE)
# %<>% para que lo de la izquierda entre tambien en la funcion

# delete NA's
raw_data %<>% drop_na()

# Summarise
skim(raw_data)

# hemos limpiado los datos eliminando duplicados y los valores nulos y hemos repetido summarise con los datos limpios
```
# EDA
## Log salary logaritmo de la variable endógena o dependiente

```{r Log salary,fig.height = 10, fig.width = 10, fig.align = "center"}

log_data <- raw_data %>% mutate(salary=log(salary))

skim(log_data)
# Excluded vars (factor)

vars <- c("player","nba_country","tm")
# excluimos de las variables las de tipo character pues no resultan relevantes para el analisis

# Correlations
corrplot(cor(log_data %>% 
               select_at(vars(-vars)), 
             use = "complete.obs"), 
         method = "circle",type = "upper")

# Other Correlations

ggcorrplot(cor(log_data %>% 
               select_at(vars(-vars)), 
            use = "complete.obs"),
            hc.order = TRUE,
            type = "lower",  lab = TRUE)

# matriz de correlaciones de las variables numericas para ir analizando que variables explican mejor los cambios en la variable independiente salary. Parece que las mas relacionadas con el salario son: g, mp, owx, dws, ws, vorp
```

# Regularización

Se trata del procedimiento que tiene como objetivo evitar que el modelo se sobreajuste a los datos y, por lo tanto, resuelve los problemas de alta varianza.

Primero dividimos la muestra en dos submuestras de forma aleatoria, una para training (se estima el modelo) y otra para testing (se predice el modelo). Al tarining le asignamos 70% y a testing 30%.

```{r training and test}
set.seed(1234) # establecemos la semilla para la muestra
nba_split <- initial_split(raw_data, prop = 0.70, strata = "salary")
nba_train <- training(nba_split) # muesta para training
nba_test  <- testing(nba_split) # muestra para testing
```

Ahora pasamos a crear las matrices de regresores y los vectores de respuesta para training y test.

```{r model matrices and response vectors}
nba_train_x <- model.matrix(salary ~ ., nba_train)[, -1]
nba_train_y <- log(nba_train$salary)

nba_test_x <- model.matrix(salary ~ ., nba_test)[, -1]
nba_test_y <- log(nba_test$salary)

dim(nba_train_x)
# dimension de la mmatriz de regresores
```

## Método de contracción: Ridge

Este modelo hace que los coeficientes sean más pequeños aproximandolos a cero aunque sin llegar exactamente a cero. Trabajamos con alpha = 0

```{r Modelo Ridge}
nba_ridge <- glmnet(x = nba_train_x, # Matriz de regresores
  y = nba_train_y,  #Vector de la variable a predecir
  alpha = 0,     # Indicador del tipo de regularizacion
  standardize = TRUE)
# modelo Ridge donde x es la matriz de regresores, y el vector de la variable a predecir, alpha igual a cero indicando a glmnet que realice la regresión cresta, estandariza las variables independientes.

plot(nba_ridge, xvar = "lambda")
# representacion grafica de las regresiones crestas

ridge_coef= nba_ridge %>% tidy()
ridge_coef 
# dataframe con los coeficientes ordenados
```
```{r lambda}
# lambda aplicado al parametro de penalizacion para controlar el impacto de la contraccion y comprobamos los primeros datos
nba_ridge$lambda %>% head()
```

### Tuning 

Elección lambda óptimo mediante Cross validation

```{r cross validation}
nba_ridge_cv <- cv.glmnet(x = nba_train_x,
  y = nba_train_y,
  alpha = 0,
  standardize = TRUE)

plot(nba_ridge_cv)
# grafico base, el gráfico nos muestra la media del MSE con su limite superior e inferior y la cantidad de varaibles que sobreviven para cada valor de lambda
```

```{r minimos}
min(nba_ridge_cv$cvm) 
# 1.6512 es el minimo error cuadratico medio
nba_ridge_cv$lambda.min
# 9.0455 es el lamba para el minimo error cuadratico medio, lambda optimo
log(nba_ridge_cv$lambda.min)
# 2.20 logaritmo del lamba optimo
nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]
# 1.7619 primer error estandar del minimo error cuadratico medio
nba_ridge_cv$lambda.1se 
# 15.088 lambda para este primer error estandar del minimo error cuadratico medio
log(nba_ridge_cv$lambda.1se)
# 2.7139 logaritmo 

plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")
# representacion grafica del lamba optimo que minima el error
```

### Variables inflyentes

La regresión ridge conserva todas las variables, por ello posteriormente haremos el modelo Lasso que supera esta desventaja.

```{r}
coef(nba_ridge_cv, s = "lambda.1se") %>% 
  broom::tidy() %>% 
  filter(row !="(Intercept)") %>% 
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") + 
  ylab(NULL)
```

## Método de contracción: Lasso

El método Lasso nos primite superar la desventaja del método Ridge, el cual no puede forzar los coeficientes de los predictores a exactamente 0 por mucho que intente acercarse. Lasso si es capaz de reducir los coeficientes a cero y además es bueno para la selección de las variables que explican el modelo.

Trabajamos con alpha igual a 1.

```{r Modelo Lasso}
nba_lasso <- glmnet(x = nba_train_x, # Matriz de regresores
                    y = nba_train_y, #Vector de la variable a predecir
                    alpha = 1) # Indicador del tipo de regularizacion
plot(nba_lasso, xvar = "lambda")

lasso_coef = nba_lasso %>% tidy()

lasso_coef
# nos muestra un dataframe con los coeficientes ordenados
```

### Tuning

Selección de lambda óptimo mediante validación cruzada

```{r cv}
nba_lasso_cv <- cv.glmnet(x = nba_train_x,
  y = nba_train_y,
  alpha = 1)

plot(nba_lasso_cv)
# grafico base, el gráfico nos muestra la media del MSE con su limite superior e inferior y la cantidad de varaibles que sobreviven para cada valor de lambda
```

```{r min}
min(nba_lasso_cv$cvm)
# 1.084857 es el minimo error cuadratico medio
nba_lasso_cv$lambda.min 
# 0.1115177 es el lamba para el minimo error cuadratico medio, lambda optimo
nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]
#  1.137028 primer error estandar del minimo error cuadratico medio
nba_lasso_cv$lambda.1se 
# 0.2041597 

plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.min), col = "red", lty = "dashed")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")
# graficamos los coeficientes tras Cross Validation
```

### Variables influyentes

```{r Variables a seleccionar}
coef(nba_lasso_cv, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

Según el método Lasso las variables independientes o explicativas que más influyen en la variable dependiente salary son nba_draft_number, g(partidos), mp(minutos), ws(victorias compartidas), age.

## Método de contracción: Elastic Net

Este método realiza una compensación entre la selección de variables y los coeficientes pequeños. 

Trabajamos con alpha igual a 0.5 pues este regula la importancia de cada penalización, cuanto más cerca de cero será más importante la penalización de Ridge y más cerca de 1, la tipo Lasso.


```{r comparacion con Elastic Net}
lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0) 
elastic1 <- glmnet(nba_train_x, nba_train_y, alpha = 0.25) 
elastic2 <- glmnet(nba_train_x, nba_train_y, alpha = 0.75) 
ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = 0.25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = 0.75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")

# obtenemos una comparacion grafica de los 3 metodos con lambda optimizado para los distintos coeficientes, incluyendo para el metodo de Elactic Net para alpha 0.25 y alpha 0.75.
```

Seleccion lamdba y alpha óptimos

### Tuning 

```{r lambda y alpha optimos}
fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)
# mismos pliegues en todos los modelos

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA)
tuning_grid
# alphas optimos y lambdas optimos para cada alpha
```

```{r}
# resolvemos NA´s

for(i in seq_along(tuning_grid$alpha)) {
  
  fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
# cross validation para cada alpha
  
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid
# dataframe con los minimos errores y lambdas
```

```{r representacion alpha mse min}
  tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
  ggplot(aes(alpha, mse_min)) +
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = 0.25) +
  ggtitle("MSE ± one standard error")
# representacion
```

Como se puede observar tanto en la gráfica como en el dataframe el alpha para el mínimo error es 1, por tanto el modelo Lasso es en este caso el mejor modelo predictivo.


# Conclusiones

Tras realizar las operaciones necesarias relativas a regularización y validación cruzada concluimos que las variables explicativas que mejor predicen el modelo, donde la variable salary es la variable a explicar son: nba_draft_number, g, mp, ws, age, pues del análisis ha resultado que el mejor modelo predictivo es Lasso de alpha 1.

# Referencias

* Chapter 6 Regularized Regression. Disponible en: https://bradleyboehmke.github.io/HOML/regularized-regression.html

* Queralt, R. (2020). Apuntes asignatura Predicción tema de Regresión II. Colegio Universitario de Estudios Financieros.


